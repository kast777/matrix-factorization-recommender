# Fatoriza√ß√£o de Matrizes em Sistemas de Recomenda√ß√£o: Otimiza√ß√£o de Modelos Latentes e An√°lise de Escala

Este projeto foi desenvolvido no √¢mbito da Unidade Curricular de **M√©todos Matem√°ticos em Intelig√™ncia Artificial** do Mestrado em **Matem√°tica Aplicada para a Ind√∫stria** ministrado pelo **ISEL** (Instituto Superior de Engenharia de Lisboa).

> **Classifica√ß√£o Final do Trabalho: 20 / 20**

---

## üéØ Objetivo

O foco central deste trabalho foi a explora√ß√£o de modelos de filtragem colaborativa baseados em **Fatoriza√ß√£o de Matrizes**. O desafio consistiu em implementar, otimizar e comparar modelos de **NMF** (Non-negative Matrix Factorization) e **WMF** (Weighted Matrix Factorization) num cen√°rio de **feedback impl√≠cito**, utilizando o dataset MovieLens 25M (+25 milh√µes de intera√ß√µes) em ambiente de hardware restrito.

---

## üî¨ An√°lise de Performance e Hiperpar√¢metros

### 1. O Custo da Explora√ß√£o: Manual vs. Otimizado

A narrativa central deste projeto destaca o impacto da efici√™ncia algor√≠tmica na fase de experimenta√ß√£o:

* **Implementa√ß√£o Manual (NumPy + SGD):** A natureza estoc√°stica do SGD faz com que o modelo n√£o convirja para um ponto fixo, mas sim para um intervalo de estabilidade "ruidoso". O custo computacional de realizar uma **Grid Search de 16 combina√ß√µes** de hiperpar√¢metros atingiu as **5 horas**, evidenciando a lentid√£o dos ciclos `for` em Python para grandes volumes de dados.
* **Otimizada (Implicit/Scikit-Learn):** Atrav√©s de bibliotecas que utilizam **ALS** (Alternating Least Squares) e rotinas em C++/Cython, essa explora√ß√£o √© reduzida para **segundos**, permitindo uma itera√ß√£o cient√≠fica muito mais √°gil.

### 2. Warm Start vs. Cold Start e a Divis√£o de Dados

Uma das li√ß√µes fundamentais foi a corre√ß√£o da metodologia de teste:

* Inicialmente, uma divis√£o por utilizadores gerou um cen√°rio de **Cold Start**, onde o NMF falhava por n√£o possuir perfis latentes para novos utilizadores.
* A transi√ß√£o para uma **Divis√£o por Intera√ß√µes (Warm Start)** permitiu que o modelo utilizasse o conhecimento pr√©vio dos utilizadores para prever os itens "escondidos", elevando a Precision@10 de patamares marginais para resultados competitivos.

### 3. RMSE vs. Precision@K: O Paradoxo do Erro

Observou-se que a minimiza√ß√£o do erro quadr√°tico (RMSE) nem sempre corrobora a qualidade da recomenda√ß√£o. Em sistemas de feedback impl√≠cito, a **ordena√ß√£o relativa (Ranking)** √© mais valiosa que a precis√£o do valor previsto. O ranking estabiliza a capacidade de sugest√£o muito antes do erro atingir o seu patamar m√≠nimo de oscila√ß√£o.

---

## ‚öôÔ∏è Metodologia e Implementa√ß√£o

### Engenharia de Dados e Esparsidade

* **Feedback Impl√≠cito:** Tratamos a aus√™ncia de dados n√£o como "desagrado", mas como incerteza. O **WMF** revelou-se superior ao introduzir pesos diferenciados para intera√ß√µes observadas e n√£o observadas.
* **Amostragem Negativa:** No WMF manual, a implementa√ß√£o de *Negative Sampling* foi crucial para ensinar o modelo a distinguir entre o que o utilizador consome e o vasto universo de itens n√£o interagidos.

### Interpretabilidade Latente (t-SNE)

Para validar a aprendizagem, aplic√°mos **t-SNE** sobre a matriz de itens . O resultado revelou agrupamentos geom√©tricos coerentes: filmes do mesmo g√©nero aglomeraram-se no espa√ßo latente sem que o algoritmo tivesse acesso a qualquer metadado (t√≠tulos ou categorias) durante o treino.

---

## ‚ö†Ô∏è Reflex√£o Cr√≠tica: Li√ß√µes de um Projeto de Escala

Este trabalho refor√ßou que, ao lidar com 25 milh√µes de intera√ß√µes, a **infraestrutura domina a teoria**:

1. **Otimiza√ß√£o √© Viabilidade:** O abismo temporal entre a explora√ß√£o manual e a otimizada define se um projeto √© academicamente interessante ou industrialmente aplic√°vel.
2. **Ru√≠do Estoc√°stico:** Aceitar que o SGD "estaciona" num intervalo de erro e n√£o num valor exato √© fundamental para definir crit√©rios de paragem (*early stopping*).
3. **Hardware como Restri√ß√£o:** A necessidade de gerir mem√≥ria for√ßou a utiliza√ß√£o de matrizes esparsas e opera√ß√µes vetorizadas, compet√™ncias essenciais para qualquer Engenheiro de ML.

---

## üõ†Ô∏è Tecnologias Utilizadas

* **Core:** Python, NumPy, Pandas, SciPy.
* **ML:** Scikit-Learn, Implicit (ALS).
* **Visualiza√ß√£o:** Matplotlib, Seaborn, t-SNE.

---

## üìñ Relat√≥rio Completo
O estudo detalhado, incluindo a fundamenta√ß√£o matem√°tica (decomposi√ß√£o matricial e gradiente descendente), est√° dispon√≠vel em [PDF](./docs/Fatorizacao_de_Matrizes_em_Sistemas_de_Recomendacao.pdf).
